{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "learning_rate, batch_size = 0.001, 32\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "grad_clip = None\n",
    "log_interval = 100\n",
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib import text\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/home/mcy/Programs/journalistic_quality/wiki.multi.en.vec'\n",
    "my_embedding = text.embedding.CustomEmbedding(src_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment analysis dataset -- IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    try:\n",
    "        data, subj, pos, neg = x\n",
    "        data = my_embedding.to_indices(length_clip(tokenizer(data)))\n",
    "        subj = int((int(subj) == 1))\n",
    "        return data, subj\n",
    "    except:\n",
    "        print(x)\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nlp.data.TSVDataset('data/train.csv', num_discard_samples=1)\n",
    "test_dataset = nlp.data.TSVDataset('data/test.csv', num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize using spaCy...\n",
      "Done! Tokenizing Time=1.07s, #Sentences=10024\n",
      "Done! Tokenizing Time=0.59s, #Sentences=2595\n"
     ]
    }
   ],
   "source": [
    "print('Tokenize using spaCy...')\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=10024, batch_num=253\n",
      "  key=[27, 48, 69, 90, 111, 132, 153, 174, 195, 216]\n",
      "  cnt=[5796, 3361, 679, 108, 37, 22, 6, 8, 3, 4]\n",
      "  batch_size=[51, 32, 32, 32, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader():\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePooling(gluon.HybridBlock):\n",
    "    # Mean pooling layer for output of LSTMs\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(AveragePooling, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, length):\n",
    "        # Data will have shape (T, N, C)\n",
    "        # fix the data into a certain length\n",
    "        maskedData = F.SequenceMask(data,sequence_length=length,use_sequence_length=True)\n",
    "        # average the data\n",
    "        avgState = F.broadcast_div(F.sum(maskedData, axis=0),F.expand_dims(length, axis=1))\n",
    "        return avgState\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    # contruct the network\n",
    "    def __init__(self, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            #bidirection LSTM with 200 size vector outputs\n",
    "            self.encoder = gluon.nn.HybridSequential()\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(mx.gluon.rnn.LSTM(300,num_layers=2,bidirectional=True))\n",
    "            #average the last layer\n",
    "            self.agg_layer = AveragePooling()\n",
    "            #output layer\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        encoded = self.encoder(data)  # Shape(T, N, C)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = gluon.nn.Embedding(200001, 300)\n",
    "emb.initialize(ctx=context)\n",
    "emb.weight.set_data(my_embedding.idx_to_vec.as_in_context(context))\n",
    "net = SentimentNet(dropout=dropout)\n",
    "net.hybridize()\n",
    "net.initialize(mx.init.Xavier(),ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, emb, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    #total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "    acc = mx.metric.Accuracy()\n",
    "    #print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        data = mx.nd.transpose(data.as_in_context(context))\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net(emb(data).detach(), valid_length)\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        #total_correct_num += (pred == label).sum().asscalar()\n",
    "        acc.update(preds = output, labels = label)\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    #acc = total_correct_num / float(total_sample_num)\n",
    "    return avg_L, acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, emb, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'Adam',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            with autograd.record():\n",
    "                output = net(emb(data.as_in_context(context).T).detach(),\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, emb, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.2f}, '\n",
    "              'test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                  epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n",
    "        file_name = \"weights/epoch{}.params\".format(epoch)\n",
    "        net.save_parameters(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 100/253] elapsed 1.68 s, avg loss 0.016214, throughput 63.61K wps\n",
      "[Epoch 0 Batch 200/253] elapsed 1.69 s, avg loss 0.013115, throughput 65.27K wps\n",
      "[Epoch 0] train avg loss 0.014509, test acc 0.52, test avg loss 0.597112, throughput 64.50K wps\n",
      "[Epoch 1 Batch 100/253] elapsed 1.67 s, avg loss 0.014016, throughput 66.27K wps\n",
      "[Epoch 1 Batch 200/253] elapsed 1.65 s, avg loss 0.013055, throughput 63.28K wps\n",
      "[Epoch 1] train avg loss 0.013724, test acc 0.52, test avg loss 0.573651, throughput 65.30K wps\n",
      "[Epoch 2 Batch 100/253] elapsed 1.64 s, avg loss 0.014041, throughput 66.98K wps\n",
      "[Epoch 2 Batch 200/253] elapsed 1.58 s, avg loss 0.013522, throughput 64.13K wps\n",
      "[Epoch 2] train avg loss 0.013485, test acc 0.52, test avg loss 0.563522, throughput 66.00K wps\n",
      "[Epoch 3 Batch 100/253] elapsed 1.57 s, avg loss 0.013502, throughput 67.51K wps\n",
      "[Epoch 3 Batch 200/253] elapsed 1.62 s, avg loss 0.012817, throughput 67.05K wps\n",
      "[Epoch 3] train avg loss 0.013309, test acc 0.52, test avg loss 0.566868, throughput 67.18K wps\n",
      "[Epoch 4 Batch 100/253] elapsed 1.61 s, avg loss 0.013221, throughput 69.56K wps\n",
      "[Epoch 4 Batch 200/253] elapsed 1.58 s, avg loss 0.013270, throughput 64.49K wps\n",
      "[Epoch 4] train avg loss 0.013129, test acc 0.52, test avg loss 0.552127, throughput 66.92K wps\n",
      "[Epoch 5 Batch 100/253] elapsed 1.64 s, avg loss 0.012169, throughput 66.50K wps\n",
      "[Epoch 5 Batch 200/253] elapsed 1.57 s, avg loss 0.013984, throughput 65.36K wps\n",
      "[Epoch 5] train avg loss 0.013075, test acc 0.52, test avg loss 0.564012, throughput 66.19K wps\n",
      "[Epoch 6 Batch 100/253] elapsed 1.58 s, avg loss 0.013508, throughput 66.69K wps\n",
      "[Epoch 6 Batch 200/253] elapsed 1.62 s, avg loss 0.013070, throughput 67.19K wps\n",
      "[Epoch 6] train avg loss 0.012996, test acc 0.52, test avg loss 0.574331, throughput 66.51K wps\n",
      "[Epoch 7 Batch 100/253] elapsed 1.62 s, avg loss 0.012511, throughput 65.83K wps\n",
      "[Epoch 7 Batch 200/253] elapsed 1.63 s, avg loss 0.013318, throughput 65.86K wps\n",
      "[Epoch 7] train avg loss 0.013037, test acc 0.52, test avg loss 0.562190, throughput 65.76K wps\n",
      "[Epoch 8 Batch 100/253] elapsed 1.64 s, avg loss 0.012493, throughput 69.05K wps\n",
      "[Epoch 8 Batch 200/253] elapsed 1.62 s, avg loss 0.013044, throughput 64.12K wps\n",
      "[Epoch 8] train avg loss 0.012694, test acc 0.52, test avg loss 0.548742, throughput 65.48K wps\n",
      "[Epoch 9 Batch 100/253] elapsed 1.65 s, avg loss 0.012741, throughput 65.05K wps\n",
      "[Epoch 9 Batch 200/253] elapsed 1.66 s, avg loss 0.012195, throughput 66.76K wps\n",
      "[Epoch 9] train avg loss 0.012537, test acc 0.52, test avg loss 0.567904, throughput 64.96K wps\n"
     ]
    }
   ],
   "source": [
    "train(net, emb, context, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
