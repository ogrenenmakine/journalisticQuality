{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "mx.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "language_model_name = 'standard_lstm_lm_200'\n",
    "pretrained = True\n",
    "learning_rate, batch_size = 0.001, 32\n",
    "bucket_num, bucket_ratio = 10, 0.2\n",
    "grad_clip = None\n",
    "log_interval = 100\n",
    "context = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib import text\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model, vocab = nlp.model.get_model(name=language_model_name,\n",
    "                                      dataset_name='wikitext-2',\n",
    "                                      pretrained=pretrained,\n",
    "                                      ctx=context,\n",
    "                                      dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentiment analysis dataset -- IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 500.\n",
    "length_clip = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(x):\n",
    "    try:\n",
    "        data, subj, pos, neg = x\n",
    "        data = vocab[length_clip(tokenizer(data))]\n",
    "        subj = int((int(subj) == 1))\n",
    "        return data, subj\n",
    "    except:\n",
    "        print(x)\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = nlp.data.TSVDataset('data/train.csv', num_discard_samples=1)\n",
    "test_dataset = nlp.data.TSVDataset('data/test.csv', num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenize using spaCy...')\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.SimpleDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.SimpleDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "test_dataset, test_data_lengths = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    # Construct the DataLoader\n",
    "    # Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, ret_length=True),\n",
    "        nlp.data.batchify.Stack(dtype='float32'))\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    test_dataloader = gluon.data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePooling(gluon.HybridBlock):\n",
    "    # Mean pooling layer for output of LSTMs\n",
    "    def __init__(self, prefix=None, params=None):\n",
    "        super(AveragePooling, self).__init__(prefix=prefix, params=params)\n",
    "\n",
    "    def hybrid_forward(self, F, data, length):\n",
    "        # Data will have shape (T, N, C)\n",
    "        # fix the data into a certain length\n",
    "        maskedData = F.SequenceMask(data,sequence_length=length,use_sequence_length=True)\n",
    "        # average the data\n",
    "        avgState = F.broadcast_div(F.sum(maskedData, axis=0),F.expand_dims(length, axis=1))\n",
    "        return avgState\n",
    "\n",
    "class SentimentNet(gluon.HybridBlock):\n",
    "    # contruct the network\n",
    "    def __init__(self, dropout, prefix=None, params=None):\n",
    "        super(SentimentNet, self).__init__(prefix=prefix, params=params)\n",
    "        with self.name_scope():\n",
    "            self.embedding = None\n",
    "            #bidirection LSTM with 200 size vector outputs\n",
    "            self.encoder = gluon.nn.HybridSequential()\n",
    "            with self.encoder.name_scope():\n",
    "                self.encoder.add(mx.gluon.rnn.LSTM(200,num_layers=2,bidirectional=True))\n",
    "            #average the last layer\n",
    "            self.agg_layer = AveragePooling()\n",
    "            #output layer\n",
    "            self.output = gluon.nn.HybridSequential()\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(gluon.nn.Dropout(dropout))\n",
    "                self.output.add(gluon.nn.Dense(1, flatten=False))\n",
    "\n",
    "    def hybrid_forward(self, F, data, valid_length): # pylint: disable=arguments-differ\n",
    "        encoded = self.encoder(self.embedding(data))  # Shape(T, N, C)\n",
    "        agg_state = self.agg_layer(encoded, valid_length)\n",
    "        out = self.output(agg_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SentimentNet(dropout=dropout)\n",
    "net.hybridize()\n",
    "net.initialize(mx.init.Xavier(),ctx=context)\n",
    "net.embedding = lm_model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, dataloader, context):\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "    total_L = 0.0\n",
    "    total_sample_num = 0\n",
    "    total_correct_num = 0\n",
    "    start_log_interval_time = time.time()\n",
    "\n",
    "    print('Begin Testing...')\n",
    "    for i, ((data, valid_length), label) in enumerate(dataloader):\n",
    "        valid_length = valid_length.as_in_context(context).astype(np.float32)\n",
    "        label = label.as_in_context(context)\n",
    "        output = net((data.T.as_in_context(context)),\n",
    "                             valid_length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "\n",
    "        L = loss(output, label)\n",
    "        pred = (output > 0.5).reshape(-1)\n",
    "        total_L += L.sum().asscalar()\n",
    "        total_sample_num += label.shape[0]\n",
    "        total_correct_num += (pred == label).sum().asscalar()\n",
    "\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('[Batch {}/{}] elapsed {:.2f} s'.format(\n",
    "                i + 1, len(dataloader),\n",
    "                time.time() - start_log_interval_time))\n",
    "            start_log_interval_time = time.time()\n",
    "\n",
    "    avg_L = total_L / float(total_sample_num)\n",
    "    acc = total_correct_num / float(total_sample_num)\n",
    "\n",
    "    return avg_L, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, context, epochs):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate})\n",
    "    loss = gluon.loss.SigmoidBCELoss()\n",
    "\n",
    "    parameters = net.collect_params().values()\n",
    "\n",
    "    # Training/Testing\n",
    "    for epoch in range(epochs):\n",
    "        # Epoch training stats\n",
    "        start_epoch_time = time.time()\n",
    "        epoch_L = 0.0\n",
    "        epoch_sent_num = 0\n",
    "        epoch_wc = 0\n",
    "        # Log interval training stats\n",
    "        start_log_interval_time = time.time()\n",
    "        log_interval_wc = 0\n",
    "        log_interval_sent_num = 0\n",
    "        log_interval_L = 0.0\n",
    "\n",
    "        for i, ((data, length), label) in enumerate(train_dataloader):\n",
    "            L = 0\n",
    "            wc = length.sum().asscalar()\n",
    "            log_interval_wc += wc\n",
    "            epoch_wc += wc\n",
    "            log_interval_sent_num += data.shape[1]\n",
    "            epoch_sent_num += data.shape[1]\n",
    "            with autograd.record():\n",
    "                output = net((data.T.as_in_context(context)),\n",
    "                             length.as_in_context(context)\n",
    "                                   .astype(np.float32))\n",
    "                L = L + loss(output, label.as_in_context(context)).mean()\n",
    "            L.backward()\n",
    "            # Clip gradient\n",
    "            if grad_clip:\n",
    "                gluon.utils.clip_global_norm(\n",
    "                    [p.grad(context) for p in parameters],\n",
    "                    grad_clip)\n",
    "            # Update parameter\n",
    "            trainer.step(1)\n",
    "            log_interval_L += L.asscalar()\n",
    "            epoch_L += L.asscalar()\n",
    "            if (i + 1) % log_interval == 0:\n",
    "                print(\n",
    "                    '[Epoch {} Batch {}/{}] elapsed {:.2f} s, '\n",
    "                    'avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                        epoch, i + 1, len(train_dataloader),\n",
    "                        time.time() - start_log_interval_time,\n",
    "                        log_interval_L / log_interval_sent_num, log_interval_wc\n",
    "                        / 1000 / (time.time() - start_log_interval_time)))\n",
    "                # Clear log interval training stats\n",
    "                start_log_interval_time = time.time()\n",
    "                log_interval_wc = 0\n",
    "                log_interval_sent_num = 0\n",
    "                log_interval_L = 0\n",
    "        end_epoch_time = time.time()\n",
    "        test_avg_L, test_acc = evaluate(net, test_dataloader, context)\n",
    "        print('[Epoch {}] train avg loss {:.6f}, test acc {:.3f}, '\n",
    "              'test avg loss {:.6f}, throughput {:.2f}K wps'.format(\n",
    "                  epoch, epoch_L / epoch_sent_num, test_acc, test_avg_L,\n",
    "                  epoch_wc / 1000 / (end_epoch_time - start_epoch_time)))\n",
    "        #file_name = \"weights/epoch{}.params\".format(epoch)\n",
    "        #net.save_parameters(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(net, context, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
